common:
  # The number of historical images
  history_size: 1
  # The number of future actions to predict
  action_chunk_size: 64
  # The number of cameras to be used in the model
  num_cameras: 3
  # Dimension for state/action, we use the same space for both state and action
  # This MUST be equal to configs/state_vec.py
  state_dim: 128


dataset:
  # The path to the buffer
  data_pth: data/finetune_new/
  # We will filter the episodes with length less than `epsd_len_thresh_low`
  # epsd_len_thresh_low: 32
  # For those more than `epsd_len_thresh_high`,
  # we will randomly sample `epsd_len_thresh_high` steps each time we load the episode
  # to better balance the training datasets
  # epsd_len_thresh_high: 2048
  # How to fit the image size
  image_aspect_ratio: pad
  pcd_min_num: 8192
  pcd_max_num: 8192
  pcd_noise_std: 0.005

model:
  # Config for condition adpators
  pcd_adaptor: mlp2x_gelu
  pcd_embed_adaptor: mlp2x_gelu
  img_adaptor: mlp2x_gelu
  vision_adaptor: mlp2x_gelu
  state_adaptor: mlp3x_gelu
  pcd_token_dim: 384
  img_token_dim: 1024
  pcd_embed_dim: 512
  vision_token_dim: 1024
  global_cond_token_len: 1344
  # Dim of action or proprioception vector
  # A `state` refers to an action or a proprioception vector
  state_token_dim: 128
  # Config for RDT structure
  rdp:
    # 1B: num_head 32 hidden_size 2048
    hidden_size: 2048
    depth: 28
    num_heads: 32
    cond_pos_embed_type: multimodal 
  # For noise scheduler
  noise_scheduler:
    type: ddpm
    num_train_timesteps: 1000
    num_inference_timesteps: 5
    beta_schedule: squaredcos_cap_v2  # Critical choice
    prediction_type: sample
    clip_sample: False
  # For EMA (params averaging)
  # We do not use EMA currently
  ema:
    update_after_step: 0
    inv_gamma: 1.0
    power: 0.75
    min_value: 0.0
    max_value: 0.9999
